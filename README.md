# ğŸ“° Text Classification under 1 million parameters: Fine-Tuning RoBERTa with LoRA on AGNEWS

**CS-GY 6953 / ECE-GY 7123 - Deep Learning Project 2 (Spring 2025)**  
**New York University (NYU) Tandon School of Engineering**

---

## ğŸ“Œ Overview

This repository contains an efficient fine-tuning implementation of the `roberta-base` model using **Low-Rank Adaptation (LoRA)** for text classification on the **AGNEWS** dataset. The goal is to achieve **high accuracy** with **fewer than 1 million trainable parameters**.

---

## ğŸ§  Model Architecture

- **Base Model**: [`roberta-base`](https://huggingface.co/roberta-base)
- **LoRA Applied To**:
  - `self-attention.query`
  - `self-attention.value`
  - `classifier.dense`
- **LoRA Configuration**:
  - `rank (r)`: 1
  - `alpha`: 2
  - `dropout`: 0.5
  - `bias`: none
- **Frozen Parameters**: All pre-trained RoBERTa parameters
- **Trainable Components**: LoRA adapters + classifier head
- **Classifier Head**: Dense â†’ Output layer (4 classes)

### ğŸ”¢ Trainable Parameters
- **Total**: 632,068  
- **Percentage**: 0.5045% of the full model (125M parameters)

---

## ğŸ› ï¸ Training Methodology

### ğŸ§° Framework & Tools
- **Libraries**: PyTorch, Hugging Face Transformers, [PEFT](https://github.com/huggingface/peft)
- **Hardware**: GPU acceleration with **mixed precision** (fp16=True)

### ğŸ§¼ Data Preprocessing
- **Dataset**: [AGNEWS](https://huggingface.co/datasets/ag_news) (4 classes of news articles)
- **Tokenizer**: `RobertaTokenizer`
  - Max length: 256 tokens
  - Truncation and dynamic padding
- **Split**: 90% Train / 10% Validation

### âš™ï¸ Optimization
- **Optimizer**: Adam
- **Learning Rate**: 7e-4
- **Weight Decay**: 0.003

### ğŸ“Š Training Configuration
- **Epochs**: 3
- **Batch Size**: 32 (train), 64 (validation)
- **Early Stopping**: Enabled (patience = 3)
- **Mixed Precision Training**: Enabled

---

## ğŸ“ˆ Results & Performance

| Metric                  | Value         |
|------------------------|---------------|
| âœ… Validation Accuracy | **94.44%**    |
| ğŸ”¥ Training Loss       | 0.156         |
| ğŸ§® Trainable Params    | 632,068       |
| â± Runtime              | ~12 minutes   |
| âš¡ Throughput (samples/sec) | 442.1   |
| âš¡ Throughput (steps/sec)   | 13.82   |

---

## ğŸ” Evaluation & Inference

- Predictions on the test set were generated using the `DL_Project_2_final.ipynb` notebook.
- Final predictions saved in: `project2_best_submission.csv` (CSV format).

---

## ğŸ“‚ Repository Structure

```bash
.
â”œâ”€â”€ DL_Project_2_final.ipynb                # Training and fine-tuning script with LoRA
â”œâ”€â”€ README.md                               # Project documentation
â””â”€â”€ project2_best_submission.csv            # Final test predictions
